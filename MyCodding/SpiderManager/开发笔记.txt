2017.1.3
项目名称：
 基于scrapyd的分布式爬虫管理项目
功能要求：
 使用javaweb技术实现一个爬虫任务的管理程序，完成对爬虫任务的增、删、改、查功能


数据库名scrapy
爬虫任务表定义crawltask;
爬虫标识 id
爬虫项目名 project
爬虫名称 name
爬虫状态 status   //-1 删除  0 无法访问  1 运行 2 结束 3 挂起（pending） 
爬虫任务jobid （提交给scrapyd时生成）
爬虫项目所在服务器网址url    （必须和提交项目下的scrapy.cfg中的url定义一致）

客户----我们的应用（javatomcat）--------scrapyd（管理运行爬虫）
         (爬虫任务表)         (http\json)
客户通过页面，输入项目名、爬虫名、爬虫项目压缩文件（zip），提交给“我们的应用”，应用后台执行提交能执行爬虫的scrapyd ,从scrapyd获取返回信息。

难点：
1 应用与scrapy的交互接口
  scrapyd.pdf  的api 接口，http协议post/get提交，返回的是json格式的文本

 a java应用能提交http请求
 b 能获取解析json格式的信息

2 如何将爬虫程序提交到scrapyd
通过专门的命令行工具
a java应用调用命令行工具，执行命令，提交爬虫程序文件

添加爬虫：
1 用户输入：
  项目名、爬虫名、爬虫项目压缩文件
2 用户输出：
  如果成功 ，给出提示信息。

删除与浏览：
首先要浏览爬虫信息
删除采用软删除：将status改为-1
   
浏览：
 列表显示（status不能是-1的项）

id  项目名  爬虫名  爬虫id   状态    链接
1   51job    it1     1323    运行    停止 链接  启动链接   删除链接    

登陆页面：
1 简单登陆页面

用户表user：
id 、用户名、密码、email 




第一轮任务：
统一用MVC做
1 李智超：登陆、添加爬虫
2 薛加志、李轩峰  ：浏览与删除
3 邹涛、徐耀辉、方宏：
   a java 模拟 http post提交 获取响应
   b json格式信息的解析
   c java 调用 命令 exec
   d 一个周期执行的任务，查询scrapyd的状态，将状态写入数据库



b json格式信息的解析
 需要导入json-lib-2.4-jdk15.jar,学着用此jar包，提出scrapyd响应json格式字符串中的相应关键字值，
需要上网进一步检索资料
 参考代码：
//将java对象转成json字符串	
	public static String getJSONString(Object obj){
		return JSONObject.fromObject(obj).toString();	
	}
	//将json字符串，转成java对象
	public static PredictResult getJSONObject(String jsonstr){
		JSONObject jsonObject = JSONObject.fromObject(jsonstr); 
		return (PredictResult)JSONObject.toBean(jsonObject,PredictResult.class); 
	}

d 周期任务（参考SSOMonitor）：
实现了一个监控程序，输入一个url ，输出一个文本文件/web-inf/ini/states.ini , 0正常，1异常；
需要配置的参数文件 ：/web-inf/ini/sso.ini   超时（ms）、扫描间隔、监控地址url。
servlet 2.4 ，集成到现有web应用中，服务器初始启动时就加载。
可以扩展该项目代码，周期性查询scrapyd，获取爬虫任务状态信息

c java 模拟 http post提交 获取响应
  网络检索参考HttpClient组件
参考代码：
  HttpPost sendRequest = new HttpPost("http://paper.people.com.cn/pdfcheck/check/checkCode.jsp");
		      List<NameValuePair> sendRequestData = new ArrayList<NameValuePair>();
		      sendRequestData.add(new BasicNameValuePair("checkCode", code));
		      String str = pdfurl.substring(pdfurl.indexOf("page"),pdfurl.length());
		      sendRequestData.add(new BasicNameValuePair("filename", str));
		      sendRequest.setEntity(new UrlEncodedFormEntity(sendRequestData, HTTP.UTF_8));
		      System.out.println(sendRequest.getRequestLine());
		      HttpResponse sendResponse = httpClient.execute(sendRequest);  



2017.1.4
实现了window7 下 使用scrapyd-deploy.bat 上传、启动hrtencent的功能。
 执行 ：
  scrapyd-deploy -p hrtencent -v r1.0.0
  上传项目，响应：
{"status": "ok", "project": "hrtencent", "version": "r1.0.0", "spiders": 1, "nod
e_name": "55J42TLEZUEXYJU"}
 执行启动爬虫命令（http post方式）：
  F:\pywork\PycharmProjects\TencentHR\hrtencent>curl http://localhost:6800/schedul
e.json -d project=hrtencent -d spider=hrtencent
  启动上传的项目爬虫，响应：
{"status": "ok", "jobid": "59b3bbf0d24011e6bcb5b870f404fa8e", "node_name": "55J4
2TLEZUEXYJU"}
 停止任务的命令及响应（post）：
F:\pywork\PycharmProjects\TencentHR\hrtencent>curl http://localhost:6800/cancel.
json -d project=hrtencent -d job=59b3bbf0d24011e6bcb5b870f404fa8e
{"status": "ok", "prevstate": null, "node_name": "55J42TLEZUEXYJU"}

 获取爬虫状态（get方式）：
F:\pywork\PycharmProjects\TencentHR\hrtencent>curl http://localhost:6800/listjob
s.json?project=hrtencent
{"status": "ok", "running": [], "finished": [{"start_time": "2017-01-04 13:40:50
.633000", "end_time": "2017-01-04 13:41:05.084000", "id": "59b3bbf0d24011e6bcb5b
870f404fa8e", "spider": "hrtencent"}], "pending": [], "node_name": "55J42TLEZUEX
YJU"}

下午实现了java通过http发送get、post请求给scrapyd的代码     

继续实现json返回信息的解析：
目标：
将scrapyd返回的json格式字符串，解析成java对象，提取出其中的信息

1 使用Json-lib库，需要的 jar 包：
    commons-beanutils-1.8.3.jar
    commons-collections-3.2.1.jar
    commons-lang-2.6.jar
    commons-logging-1.1.1.jar
    ezmorph-1.0.6.jar
    json-lib-2.4-jdk15.jar
需要解析出简单json字符串和含数组结构的字符串

启动、停止任务返回的json信息可以转为Map 键值对对象，已实现

查询爬虫状态返回的是复杂json格式，需要继续实现。

2 java 调用 批处理程序(bat)  将项目上传到scrapyd 
  a 要能将当前路径切换到上传爬虫所在的路径
    /tmp/hrtencent/
  b 执行scrapyd-deploy.bat 并传入多个参数

2017.1.5
管理程序与scrapyd之间接口的设计实现：
采用分层设计的思路，从最底层http通信层开始实现，增量迭代开发
在spider.scrapyd下：
上层（3实现了主要功能api接口）：
ScrapydDao.java   提供了部署、启停、获取爬虫状态的高层接口，部署接口尚未实现
中间层(2已实现，协议层)：
 Json.java 通过解析json格式字符串的接口（xxStatus则是值对象类，对应scrapyd对外提供的状态信息）
底层（1已实现,通信层）：
 HttpPost.java  与scrapyd直接通信的接口，实现了get、post方式通信

目前需要实现的是：
1 java exec 调用 scrapyd-deploy.bat 将项目上传到scrapyd
 a 爬虫项目路径：/upload/hrcent/hrcent ,先切换路径到第一个hrcent,要把
  /upload/hrcent作为当前工作路径 ，
 b java exec 中需要传递参数：目标项目名“hrcent”，版本号 "r1.0.0"
 c 获取命令执行的返回结果
 学习exec的用法。
 
2 在 MonitorTask中扩展原代码功能：
实现一个方法能获取scrapyd中爬虫的状态信息，解析信息，更新crawltask中爬虫的status
a 从crawltask中获取有效的spider记录信息
b 调用ScrapydDao中的getStatus方法,需要项目名和jobid
c 解析返回的JsonStatus对象信息，更新数据表中爬虫的status

2016.1.6
1 实现了java exec 调用 scrapyd-deploy.bat （可以指定工作路径） ，测试通过
2 实现了ScrapydDao.java中的部署爬虫方法 deploySpider、获取状态的方法getStatus、启动停止爬虫的方法
startSpider、stopSpider，删除项目的方法delProject,每个方法均在scrapyd正常运行和停止的情况下测试通过。
（重复部署同一爬虫项目，不会出问题；删除项目后，eggs下项目文件被删除，但logs下的日志文件还在）
3 下一步：
 a 第一轮项目集成
 b 考虑第二轮迭代需要实现的功能：
 （1）扩展crawltask表结构，添加url字段
 （2） 实现周期调度功能，由应用周期向 scrapyd发出启、停爬虫的命令           

2017.1.8
1 集成登陆，上传功能（主要采用复制文件的方式）
a 发现两个项目编码不一致，统一为utf-8编码
b 建立了spidermanager mysql库，user表（id、name、password、email）
c 修改了与数据库表操作有关的代码
d 修正了上传文件中的一个bug 
    上传jsp的form中加 enctype="multipart/form-data"

2017.1.9
集成中遇到的问题：
1 应该明确统一数据库名、表名、字段名
2 应该明确统一包名
3 明确jsp与servlet的接口，通过session传递的对象定义，如包含爬虫任务对象的列表 
 ArrayList<CrawlTask> ls = new ArrayList<CrawlTask>

